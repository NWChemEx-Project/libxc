# LibXC for GPU

We want to have LibXC run on GPUs so that all the DFT related work can be kept
on the device and communication can be avoided. In our approach we use CUDA
because that gives us the most control. For the inputs the density and its
derivatives are evaluated on the GPU. In addition the derivatives of the
functional are consumed on the GPU as well. 

LibXC manages the details of the functional in an intricate way. Dealing with
this on the GPU would cause a number of issues. Therefore we do not want to
deal with this on the device. Instead we propose to handle the logic of 
the functional entirely on the host. Just when we get to actually evaluating
the functional we want to launch a kernel on the GPU to do this.

Implementing the above approach requires a number of steps.

## Extending xc_func_info_type

xc_func_info_type holds a variety of data about the functional. This information
includes function pointers to the code that evaluates the functional. In the
original version of LibXC those pointers are called `lda`, `gga`, and `mgga`
for function pointers with a corresponding API. 

For the GPU implementation `lda`, `gga`, and `mgga` will remain the same 
except that they will now point to `__host__` versions of the functional code.
To deal with the device code we add `lda_offload`, `gga_offload`, and
`mgga_offload` that point to `__global__` functions that invoke the device
implementations of the functional. If the code is compiled without GPU 
support the pointers `lda_offload`, `gga_offload`, and `mgga_offload` will
point to `NULL`. 

## Extending the functional implementations

The code for the functionals is in general stored in C files. The Nvidia
compiler, however, insists on CUDA code. To avoid duplicating all the code
most CUDA is "created" through using soft links, for example
```
   mgga_c_tpss.cu -> ./mgga_c_tpss.c
```
In a few cases the functional needs to call code that needs separate CUDA
implementations. In those special cases actual separate copies of the files
for the CUDA code are created. For the most part these files are lightly 
modified copies of the original. Examples of these kinds of files are
```
   gga.cu
   gga_x_lb.cu
   lda.cu
   mgga.cu
   mix_func.cu
```

The code for the functionals is generated by extensively using function 
inlining. The function is provided by Maple generated code which is included
into a file that captures all the important functional material. The
functional is inlined into a `work_<functionalType>` routine (e.g. 
`work_lda`). Pointers to this code are stored in a `xc_func_info_type`
data structure.

For the GPU implementation the `work_lda` function will be declared as 
`__host__ __device__` to generate CPU and GPU versions of this code. A pointer
to the CPU code will be stored in `xc_func_info_type` just like with the 
original code. However we cannot do the same for the GPU code. Instead we
need to add a `__host__ work_lda_offload` function that will launch the 
device code on the GPU through a call to `__global__ work_lda_device`.
The pointer to this offload function will be stored
in `xc_func_info_type.lda_offload`. 

## Dealing with xc_func_type

The function `xc_func_init` sets the definition of the functional up 
dependent on the number ID of the functional requested. The data is stored 
in an `xc_func_type` data structure. The problem with this approach is that
sending parameters for the functional over to the GPU for every kernel 
invocation is likely prohibitively expensive. Duplicating the `xc_func_type`
on the device is also problematice because it stores the functional 
definition as a tree of multiple functional terms. Also the original 
`xc_func_type` data structure contained several pointers to other
data structures. This means that copying this data structure over to the
device requires rebuilding the whole pointer structure on the device as 
well.

The approach taken here was to remove most of the pointers from 
`xc_func_type` by replacing them with fixed size arrays. This
makes that `xc_func_type` contains most information needed on the device
without any pointers. In the function `xc_func_init_device` we build an array
`xc_func_data_host` of `xc_func_type` with an entry for every functional.
We also build another array `xc_func_info_data_host` of `xc_func_info_type`
that holds a copy of all functional info for all functionals. Memory for 
copies of `xc_func_data_host` and `xc_func_info_data_host` are allocated on 
the device with the obvious names `xc_func_data_device` and 
`xc_func_info_data_device`. The pointers to the functional info in
`xc_func_data_host` are changed to that they are correct on the device
side by making them point into `xc_func_info_data_device`.
After that `xc_func_data_host` and `xc_func_info_data_host` can be copied
to their device counter parts `xc_func_data_device` and 
`xc_func_info_data_device`, and now all relevant data is available on the
device.

A new function `xc_func_init_all` initializes all functionals (`xc_func_end_all`
finalizes all functionals). This function also ensures that all relevant data
is copied to the device.

In order to make manipulating these tables of functionals easy a new 
function `xc_functional_get_rank` was created that returns the position
of a functional in the `xc_functional_keys` table. This rank is
stored in a new `xc_func_type.func_rank` field.

## Mixing functionals

A significant number of functionals consist out of a number of terms 
that are each functionals in their own right. Each functional term
needs to be evaluated and added onto the total functional. In the CPU
code memory for the functional terms is allocated. On the GPU such 
memory allocation operations are device synchronizing operations
with significant performance consequences. Hence, in the function
`xc_mix_func_offload` we assume that all the buffers for the results
are twice as long as what is needed for the results alone. The second
half of each array is used as the buffer to store the results for
each term. This way the memory allocation is moved out of the functional
evaluation.

## Building the code for the GPU

Building LibXC for the GPU has a number of challenges. First of all
LibXC has GPU code spread over multiple files. To compile this correctly
you need to generate relocatable device code. This requires special 
compiler flags, one of the following two options will do the trick:
```
   -rdc true
   --relocatable-device-code true
```
Unfortunately CMake does not handle this very well. CMake knows about this
concept as SEPARABLE_COMPILATION, but I have had plenty of trouble to get
that to add the proper compiler flags. In practice I have to hand edit
`CMakeCache.txt` to add `-rdc true` to `CMAKE_CUDA_FLAGS`.

Another issue is that GPUs are very limited on the number of registers they
have available. Compiling GPU code with the default settings is likely to
produce code that will fail to launch on the device because there are not
enough resources. Therefore it is adviced to also add 
```
   --maxrregcount 64 --use_fast_math -gencode arch=compute_70,code=sm_70
```
to `CMAKE_CUDA_FLAGS`.

## Example code

LibXC comes with code for regression testing. The original regression test
code lives in the `testsuite` directory. Another copy of that code has 
been created that exercises the functionals on the GPU.
```
   libxc/testsuite/xc-regression.c
   libxc/testsuite/xc-regression.cu
```
The main differences between the two versions of the code are:

- A new function `allocate_memory_device` allocates buffers on the GPU.
- A new function `copy_host_2_device` copies the functional input data from
  the host to the device memory.
- The call to `xc_func_init_all` defines all the functionals and makes
  the relevant data available on the device.
- The call to `cudaStreamCreate` sets up a stream context for the
  functional code to use.
- Calling either `xc_lda_offload`, `xc_gga_offload` or `xc_mgga_offload`
  drives the functional evaluation by launching relevant kernels on the
  device.
- The call to `cudaStreamSynchronize` waits for all work to finish.
- Calling `cudaStreamDestroy` cleans the stream context up.
- A new function `copy_device_2_host` copies the results from the functional
  from the device to the host memory.
- The call to `xc_func_end_all` cleans up all the memory associated with the
  functional definitions.
- A new function `free_memory_device` frees the device buffers.
